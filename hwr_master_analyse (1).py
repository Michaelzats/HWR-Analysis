# -*- coding: utf-8 -*-
"""HWR Master analyse.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gMatwGtGykAlwQLDOYgm_QhaLy-WYP4e

# **Analysis of the Survvey and Kaggle Data**

# HWR-Analysis
#README
Project Name: Effectiveness of Clickbait Headlines in Meta Platforms Advertising within the Skill Game Industry

Description: This project aims to analyse the data of the collected results of the survey for the master thesis of the marketing management program of HWR by Michael Zats and the dataset taken from Kaggle and accessed [Here](https://www.kaggle.com/datasets/thelazyaz/youtube-clickbait-classification/data?select=clickbait.csv).

Prerequriments
Python 3
Google Colab (for running the provided notebook)
Installation steps:

Clone the repository from Google Colab. ([https://colab.research.google.com/drive/1j-RytXdZDXCAWKHlaQdFWmRo64SJt_3Y?usp=sharing](https://colab.research.google.com/drive/1gMatwGtGykAlwQLDOYgm_QhaLy-WYP4e?usp=sharing)) In Colab click File --> Save a copy in Colab.

<img width="266" alt="Screenshot 2023-11-18 at 13 52 49" src="https://github.com/Michaelzats/HWR-Analysis/assets/92814061/fdd813d8-249c-441c-9ceb-874db7e565bd">

Install necessary Python libraries using !pip install (listed bellow)

Usage: Basic examples:

Load the datasets provided “Survey DATA Thesis - Clickbaits questions groups (8), Survey DATA Thesis - Clickbaits questions groups (7),Clickbait Data Thesis - Cleaned Data”. You can do it by clicking on the file folder icon from the right in Colab and then draging “DATASET” File into the opened part. Example how to drag “DATA.ZIP” into the file folder

<img width="176" alt="Screenshot 2023-11-18 at 13 57 53" src="https://github.com/Michaelzats/HWR-Analysis/assets/92814061/75100908-45bb-46b1-9659-fd4ddd2a54cc">



For any queries or feedback, please reach out to yfim13@gmail.com .

**Rules to execute:**

1. Having installed all bellow listed libraries.
2. Running inside of the copied colab of that colab.
3. Using as the dataset provided in the zip file
4. Running each cell after the cell, without jumping back and forth.

Libraries to install
"""

#Pandas:
!pip install pandas

#SciPy (also covers stats and other statistical functions):
!pip install scipy


#Matplotlib (for plotting, also covers matplotlib.pyplot):
!pip install matplotlib


#Seaborn (a visualization library based on matplotlib):
!pip install seaborn


#Scikit-learn (covers PCA, StandardScaler, SimpleImputer, KMeans):
!pip install scikit-learn


#Numpy (for numerical operations):
!pip install numpy


#Tabulate (for pretty-printing tabular data):
!pip install tabulate


#NLTK (Natural Language Toolkit, includes ngrams):
!pip install nltk



#Matplotlib Ticker (part of Matplotlib, no separate installation needed).

#Collections and re:

#The collections module is a built-in Python module, so you don't need to install it separately. It comes pre-installed with Python.

#Similarly, the re module for regular expressions is also a built-in Python module and does not require separate installation.

"""**Libraries**"""

import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import numpy as np
from sklearn.decomposition import PCA
from scipy.stats import chi2_contingency
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from tabulate import tabulate
import matplotlib.ticker as ticker
from scipy.stats import chisquare
from scipy.stats import ttest_1samp
from collections import Counter
import re
from nltk import ngrams
nltk.download('popular')

"""# **DATA WITH NO CHANGES**

"""

data = pd.read_csv('Survey DATA Thesis - Clickbaits questions groups (8).csv')


print(data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column
num_columns = data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Get descriptive statistics
desc_stats = data.describe()

# Determine the split point (halfway through the columns)
split_point = len(desc_stats.columns) // 2

# Split the descriptive statistics into two DataFrames
first_half_stats = desc_stats.iloc[:, :split_point]
second_half_stats = desc_stats.iloc[:, split_point:]

# Print the title for the table
print("Descriptive Statistics - Part 1")
# Print the first part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(first_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Add spacing between tables for readability
print("\n" + "-"*50 + "\n")

# Print the title for the second part of the table
print("Descriptive Statistics - Part 2")
# Print the second part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(second_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

"""# **H1 Investigating**
H1: Clickbait headlines in skill game advertisements on Facebook lead to higher click-through rates compared to non-clickbait headlines.

**Occurrences of 1, 2, and 3 in each of the specified columns (v_3, v_7, v_8, v_9)**
"""

# Assuming 'data' is your DataFrame and it's already loaded

# Select the columns to count values in
selected_columns = ['v_5', 'v_7', 'v_8', 'v_9']

# Count the occurrences of 1, 2, and 3 in these columns
value_counts = data[selected_columns].apply(pd.Series.value_counts)

# Get the counts for 1, 2, and 3 in each column
counts = value_counts.loc[[1, 2, 3]]

print(counts)


# Select the specified columns
selected_columns = data[['v_5', 'v_7', 'v_8', 'v_9']]

# Flatten the selected DataFrame into a single series
flattened_data = selected_columns.values.flatten()

# Convert to a Series to use value_counts
flattened_series = pd.Series(flattened_data)

# Count the occurrences of 1, 2, and 3
counts = flattened_series.value_counts().loc[[1, 2, 3]]

print(counts)

data['Headlines AVG Score'].describe()

data['Headlines AVG Score'].mode()[0]

data['Headlines AVG Score'].median()

"""**Chi-square statistic**
1 - clicbait
2 - Not-clickbait
3 - Neither clickbait or Not-clickbait **
"""

# Observed counts
observed_counts = [439, 507, 602]

# Expected counts if all categories are equally likely
expected_counts = [sum(observed_counts) / 3] * 3

# Perform the chi-square goodness of fit test
chi2_stat, p_value = chisquare(f_obs=observed_counts, f_exp=expected_counts)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")

# Observed counts for Clickbait and Not-clickbait
observed_counts = [439, 507]

# Expected counts if both categories are equally likely
expected_counts = [sum(observed_counts) / 2] * 2

# Perform the chi-square goodness of fit test
chi2_stat, p_value = chisquare(f_obs=observed_counts, f_exp=expected_counts)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")

"""**One-Sample t-Test Of V_14**
Klicken Sie eher auf eine Anzeige mit einer sensationellen als mit einer Standard-Schlagzeile?
H1
"""

data['v_14'].describe()

data['v_14'].mode()[0]

responses = data['v_14']

# Perform a one-sample t-test against the hypothetical average value (4.5)
t_stat, p_value = ttest_1samp(responses, popmean=4.5)

print(f"t-statistic: {t_stat}, P-value: {p_value}")

"""# **H2 Investigating**
H2: Skill game advertisements using clickbait headlines experience a higher rate of user drop-off after the initial click compared to those using straightforward headlines.

**One-Sample t-Test Of V_12**
Wenn Sie auf eine sensationell klingende Spielwerbung klicken, wie oft spielen Sie dann weiter oder beschäftigen sich mit dem Spiel?
"""

data['v_12'].describe()

data['v_12'].mode()[0]

responses = data['v_12']

# Perform a one-sample t-test against the hypothetical average value (2)
t_stat, p_value = ttest_1samp(responses, popmean=2)

print(f"t-statistic: {t_stat}, P-value: {p_value}")

"""
**One-Sample t-Test Of V_13**
Wie oft kommt es vor, dass der Inhalt eines Spiels hält, was die vielversprechende Überschrift verspricht?"""

data['v_13'].describe()

data['v_13'].mode()[0]

responses = data['v_13']

# Perform a one-sample t-test against the hypothetical average value (4.5)
t_stat, p_value = ttest_1samp(responses, popmean=4.5)

print(f"t-statistic: {t_stat}, P-value: {p_value}")

"""# **H3 Investigating**
H3: Users can differentiate advertisements on Facebook with clickbait headlines and without.

**One-Sample t-Test Of v_15**
Inwieweit glauben Sie, dass Spielewerbung mit sensationellen Schlagzeilen hält, was sie verspricht?
"""

data['v_15'].describe()

data['v_15'].mode()[0]

responses = data['v_15']

# Perform a one-sample t-test against the hypothetical average value (3)
t_stat, p_value = ttest_1samp(responses, popmean=3)

print(f"t-statistic: {t_stat}, P-value: {p_value}")

"""
**One-Sample t-Test Of v_16**
Wie gut können Sie Ihrer Meinung nach Clickbait-Schlagzeilen von allgemeinen Schlagzeilen in Anzeigen unterscheiden?"""

data['v_16'].describe()

data['v_16'].mode()[0]

responses = data['v_16']

# Perform a one-sample t-test against the hypothetical average value (4.5)
t_stat, p_value = ttest_1samp(responses, popmean=4.5)

print(f"t-statistic: {t_stat}, P-value: {p_value}")

"""# **H4 Investigating**
H4: Users express a desire for transparency and authenticity in skill game advertisements, with misleading clickbait headlines leading to potential skepticism about the game's quality or the integrity of the developer.

**One-Sample t-Test Of v_15**
Inwieweit glauben Sie, dass Spielewerbung mit sensationellen Schlagzeilen hält, was sie verspricht?
"""

data['v_15'].describe()

data['v_15'].mode()[0]

responses = data['v_15']
# Perform a one-sample t-test against the hypothetical average value (3)
t_stat, p_value = ttest_1samp(responses, popmean=3)

print(f"t-statistic: {t_stat}, P-value: {p_value}")

"""
**One-Sample t-Test Of v_17**
Wie wichtig sind für Sie Transparenz und Authentizität in der Spielwerbung?"""

data['v_17'].describe()

data['v_17'].mode()[0]

responses = data['v_17']
# Perform a one-sample t-test against the hypothetical average value (5)
t_stat, p_value = ttest_1samp(responses, popmean=5)

print(f"t-statistic: {t_stat}, P-value: {p_value}")

"""
**One-Sample t-Test Of v_18**
Wie sehr lässt eine Werbeschlagzeile an der Qualität eines Spiels oder dem Vertrauen seines Entwicklers zweifeln?"""

data['v_18'].describe()

data['v_18'].mode()[0]

responses = data['v_18']
# Perform a one-sample t-test against the hypothetical average value (4.5)
t_stat, p_value = ttest_1samp(responses, popmean=4.5)

print(f"t-statistic: {t_stat}, P-value: {p_value}")

"""# **ALL CORRELATIONS**"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


correlation_matrix = data.corr()

# Create a larger figure before generating the heatmap
plt.figure(figsize=(20, 18))

sns.heatmap(data = correlation_matrix, annot = True)
plt.show()

# Calculate the p-values and put them in a matrix form
pval_matrix = pd.DataFrame(index=data.columns, columns=data.columns)

for i in data.columns:
    for j in data.columns:
        _, pval = pearsonr(data[i], data[j])
        pval_matrix.loc[i, j] = pval

# Convert the pval_matrix to float
pval_matrix = pval_matrix.astype(float)

# Create a larger figure before generating the heatmap
plt.figure(figsize=(20, 18))

# Generate a heatmap for p-values
sns.heatmap(data=pval_matrix, annot=True)
plt.show()

"""# **ALL CORRELATIONS OF 5% AND MORE OR LESS THAN -5%**"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Calculate the correlation matrix
correlation_matrix = data.corr()

# Set correlations where the absolute value is less than 0.05 to NaN
correlation_matrix[correlation_matrix.abs() < 0.05] = np.nan

# Create a larger figure before generating the heatmap
plt.figure(figsize=(20, 18))

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

"""# **Pearson Correlation Coefficient Signficicance Testing 5% AND MORE OR LESS THAN -5%**"""

from scipy.stats import pearsonr

# Select the column for comparison
target_column = 'Headlines AVG Score'

# Ensure the target column is numeric and drop NaN values
if not pd.api.types.is_numeric_dtype(data[target_column]):
    raise ValueError(f"The target column '{target_column}' must be numeric.")

# Dropping rows where the target column or any other column is NaN
data_cleaned = data.dropna(subset=[target_column] + data.columns.tolist())

# Iterate through all other columns and test correlations
for col in data_cleaned.columns:
    if col != target_column:
        # Ensure the comparison column is numeric
        if pd.api.types.is_numeric_dtype(data_cleaned[col]):
            corr, p_value = pearsonr(data_cleaned[target_column], data_cleaned[col])

            # Check if absolute value of correlation is greater than 0.05
            if abs(corr) > 0.05:
                print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'duration'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_1'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_2'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_3'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_4'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_5'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_7'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_8'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_9'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_12'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_13'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_14'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_15'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_16'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_17'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_18'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_23'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_24'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_25'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

from scipy.stats import pearsonr

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_26'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

"""# **DATA WITH DELETING ALL THE ROWS WHERE: NO HEADLIENS ANSWER IS GIVEN IN V_3,7,8,9**"""

# Load the data
data_without_no_answers_in_v3789 = pd.read_csv('Survey DATA Thesis - Clickbaits questions groups (7).csv')

# Use the correct DataFrame name
print(data_without_no_answers_in_v3789.describe())  # Descriptive statistics

# Visualizing the distribution of each numerical column
num_columns = data_without_no_answers_in_v3789.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(data_without_no_answers_in_v3789[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

"""# **ALL CORRELATIONS**"""

# Drop rows where any cell is NaN
data_cleaned = data_without_no_answers_in_v3789.dropna()

# Calculate the correlation matrix on the cleaned data
correlation_matrix_data_without_no_answers_in_v3789 = data_cleaned.corr()

# Create a larger figure before generating the heatmap
plt.figure(figsize=(20, 18))
sns.heatmap(data=correlation_matrix_data_without_no_answers_in_v3789, annot=True)
plt.show()

correlation_matrix_data_without_no_answers_in_v3789[np.abs(correlation_matrix_data_without_no_answers_in_v3789) < 0.05] = np.nan

# Create a larger figure before generating the heatmap
plt.figure(figsize=(20, 18))

sns.heatmap(data=correlation_matrix_data_without_no_answers_in_v3789, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

# Drop rows where any cell (in any column, including the target column) is NaN
data_cleaned_correlation_matrix_data_without_no_answers_in_v3789 = data_without_no_answers_in_v3789.dropna()

# Select the column for comparison
target_column = 'Headlines AVG Score'

# Ensure the target column is in the DataFrame
if target_column not in data_cleaned_correlation_matrix_data_without_no_answers_in_v3789.columns:
    raise ValueError(f"The column '{target_column}' is not in the DataFrame.")

# Iterate through all other columns and test correlations
for col in data_cleaned_correlation_matrix_data_without_no_answers_in_v3789.columns:
    if col != target_column:
        # Ensure both columns have at least two non-NaN values
        if len(data_cleaned_correlation_matrix_data_without_no_answers_in_v3789[target_column].dropna()) > 1 and len(data_cleaned_correlation_matrix_data_without_no_answers_in_v3789[col].dropna()) > 1:
            # Calculating correlation and p-value
            corr, p_value = pearsonr(data_cleaned_correlation_matrix_data_without_no_answers_in_v3789[target_column], data_cleaned_correlation_matrix_data_without_no_answers_in_v3789[col])

            # Print every correlation and p-value
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

data_cleaned_correlation_matrix_data_without_no_answers_in_v3789['Headlines AVG Score'].describe()

data_cleaned_correlation_matrix_data_without_no_answers_in_v3789['Headlines AVG Score'].median()

data_cleaned_correlation_matrix_data_without_no_answers_in_v3789['Headlines AVG Score'].mode()[0]

"""# **DEMOGRAPHICAL DATA CHECKING (NOT Hypotheses CHECKING)**

Gender
"""

data['v_2'].describe()

# Load data
data = pd.read_csv('Survey DATA Thesis - Clickbaits questions groups (8).csv')

# Filter the DataFrame where v_2 equals 1
filtered_data = data[data['v_2'].isin([1, 2])]

print(filtered_data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column in the filtered data
num_columns = filtered_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Assuming 'data' is your DataFrame and 'v_1' is the column of interest
value_counts = data['v_2'].value_counts()

# Filter to get only the counts of 1, 2, 3, 4, and 5
selected_counts = value_counts.loc[value_counts.index.isin([1, 2, 3])]

# Calculate percentages
total = data['v_2'].count()
percentages = (selected_counts / total) * 100

# Combine counts and percentages
combined = pd.DataFrame({
    'Counts': selected_counts,
    'Percentages': percentages
})

print(combined)

plt.figure(figsize=(8, 6))
sns.histplot(data['v_2'], kde=True)
plt.title('Distribution of Genders')
plt.show()

import pandas as pd
from scipy.stats import pearsonr, chi2_contingency
from tabulate import tabulate

# Assuming 'data' is a pandas DataFrame with your data.

# Filter the DataFrame for rows where 'v_2' is either 1 or 2
filtered_data = data[data['v_2'].isin([1, 2])]

# Select the column for comparison, e.g., 'v_2'
target_column = 'v_2'

# Create a list to store our table rows
table = []

# Define ANSI escape sequence for bold text
BOLD = "\033[1m"
RESET = "\033[0m"

# Add the header row with bold
header = [f"{BOLD}Variable 1{RESET}", f"{BOLD}Variable 2{RESET}",
          f"{BOLD}Correlation coefficient{RESET}", f"{BOLD}Correlation coefficient P-Value{RESET}",
          f"{BOLD}Chi-square P-Value{RESET}"]
table.append(header)

# Iterate through all other columns and test correlations
for col in filtered_data.columns:
    if col == target_column:
        continue  # skip the comparison with itself

    # Pearson's correlation
    corr, p_corr_value = pearsonr(filtered_data[target_column], filtered_data[col])

    # Chi-square test - this assumes that the data is categorical!
    contingency_table = pd.crosstab(filtered_data[target_column], filtered_data[col])
    chi2_stat, p_chi_value, dof, ex = chi2_contingency(contingency_table)

    # Append the results to the table list
    table.append([target_column, col, f"{corr:.3f}", f"{p_corr_value:.3f}", f"{p_chi_value:.3f}"])

# Print the table with centered text and bold headers
print(tabulate(table, headers="firstrow", tablefmt="grid", colalign=("center",)*5))

"""**Gender Men**"""

# Load data
data = pd.read_csv('Survey DATA Thesis - Clickbaits questions groups (8).csv')

# Filter the DataFrame where v_2 equals 1
filtered_data = data[data['v_2'] == 1]

print(filtered_data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column in the filtered data
num_columns = filtered_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Get descriptive statistics
filtered_data = data[data['v_2'] == 1]
desc_stats = filtered_data.describe()

# Determine the split point (halfway through the columns)
split_point = len(desc_stats.columns) // 2

# Split the descriptive statistics into two DataFrames
first_half_stats = desc_stats.iloc[:, :split_point]
second_half_stats = desc_stats.iloc[:, split_point:]

# Print the title for the table
print("Descriptive Statistics - Part 1")
# Print the first part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(first_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Add spacing between tables for readability
print("\n" + "-"*50 + "\n")

# Print the title for the second part of the table
print("Descriptive Statistics - Part 2")
# Print the second part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(second_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

correlation_matrix = filtered_data.corr()

# Create a larger figure before generating the heatmap
plt.figure(figsize=(20, 18))

sns.heatmap(data = correlation_matrix, annot = True)
plt.show()

"""**Gender Women**"""

# Load data
data = pd.read_csv('Survey DATA Thesis - Clickbaits questions groups (8).csv')

# Filter the DataFrame where v_2 equals 2
filtered_data = data[data['v_2'] == 2]

print(filtered_data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column in the filtered data
num_columns = filtered_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Get descriptive statistics
filtered_data = data[data['v_2'] == 2]
desc_stats = filtered_data.describe()

# Determine the split point (halfway through the columns)
split_point = len(desc_stats.columns) // 2

# Split the descriptive statistics into two DataFrames
first_half_stats = desc_stats.iloc[:, :split_point]
second_half_stats = desc_stats.iloc[:, split_point:]

# Print the title for the table
print("Descriptive Statistics - Part 1")
# Print the first part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(first_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Add spacing between tables for readability
print("\n" + "-"*50 + "\n")

# Print the title for the second part of the table
print("Descriptive Statistics - Part 2")
# Print the second part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(second_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

"""# **Chi-square statistic Testing between genders**

Gender and Hast du schon einmal eine App aus der Facebook-Werbung heruntergeladen?
"""

# Filter the DataFrame to include only rows where v_2 has values 1 or 2
df_filtered = data[data['v_2'].isin([1, 2])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_2'], df_filtered['v_4'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between gender (1 and 2) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Gender and Wie wichtig sind für Sie Transparenz und Authentizität in der Spielwerbung?"""

# Filter the DataFrame to include only rows where v_2 has values 1 or 2
df_filtered = data[data['v_2'].isin([1, 2])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_2'], df_filtered['v_17'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between gender (1 and 2) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Gender and Wie gut können Sie Ihrer Meinung nach Clickbait-Schlagzeilen von allgemeinen Schlagzeilen in Anzeigen unterscheiden?"""

# Filter the DataFrame to include only rows where v_2 has values 1 or 2
df_filtered = data[data['v_2'].isin([1, 2])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_2'], df_filtered['v_16'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between gender (1 and 2) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Gender and Wie oft spielen Sie bei Spielen um Geld oder schließen Wetten ab?"""

# Filter the DataFrame to include only rows where v_2 has values 1 or 2
df_filtered = data[data['v_2'].isin([1, 2])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_2'], df_filtered['v_26'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between gender (1 and 2) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Gender and Wie risikofreudig würden Sie sich selbst einschätzen?"""

# Filter the DataFrame to include only rows where v_2 has values 1 or 2
df_filtered = data[data['v_2'].isin([1, 2])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_2'], df_filtered['v_23'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between gender (1 and 2) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Gender and Wie sehr würden Sie sich aufregen, wenn Sie einen Teil Ihres Geldes verlieren würden?"""

# Filter the DataFrame to include only rows where v_2 has values 1 or 2
df_filtered = data[data['v_2'].isin([1, 2])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_2'], df_filtered['v_24'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between gender (1 and 2) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Age"""

data['v_1'].describe()

# Assuming 'data' is your DataFrame and 'v_1' is the column of interest
value_counts = data['v_1'].value_counts()

# Filter to get only the counts of 1, 2, 3, 4, and 5
selected_counts = value_counts.loc[value_counts.index.isin([1, 2, 3, 4, 5])]

# Calculate percentages
total = data['v_1'].count()
percentages = (selected_counts / total) * 100

# Combine counts and percentages
combined = pd.DataFrame({
    'Counts': selected_counts,
    'Percentages': percentages
})

print(combined)

# Load data
data = pd.read_csv('Survey DATA Thesis - Clickbaits questions groups (8).csv')

# Filter the DataFrame where v_1 equals 2,3
filtered_data = data[data['v_1'].isin([2, 3])]

print(filtered_data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column in the filtered data
num_columns = filtered_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(data['v_1'], kde=True)
plt.title('Distribution of Ages')
plt.show()

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_1'

# Iterate through all other columns and test correlations
for col in data.columns:
    if col != target_column:
        corr, p_value = pearsonr(data[target_column], data[col])

        # Check if absolute value of correlation is greater than 0.05
        if abs(corr) > 0.05:
            print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")

import pandas as pd
from scipy.stats import pearsonr, chi2_contingency
from tabulate import tabulate

# Assuming 'data' is a pandas DataFrame with your data.

# Select the column for comparison, e.g., 'v_1'
target_column = 'v_1'

# Create a list to store our table rows
table = []

# Define ANSI escape sequence for bold text
BOLD = "\033[1m"
RESET = "\033[0m"

# Add the header row with bold
header = [f"{BOLD}Variable 1{RESET}", f"{BOLD}Variable 2{RESET}",
          f"{BOLD}Correlation coefficient{RESET}", f"{BOLD}Correlation coefficient P-Value{RESET}",
          f"{BOLD}Chi-square P-Value{RESET}"]
table.append(header)

# Iterate through all other columns and test correlations
for col in data.columns:
    if col == target_column:
        continue  # skip the comparison with itself

    # Pearson's correlation
    corr, p_corr_value = pearsonr(data[target_column], data[col])

    # Chi-square test - this assumes that the data is categorical!
    # Note that if the variables are not categorical, this test may not be appropriate
    contingency_table = pd.crosstab(data[target_column], data[col])
    chi2_stat, p_chi_value, dof, ex = chi2_contingency(contingency_table)

    # Append the results to the table list
    table.append([target_column, col, f"{corr:.3f}", f"{p_corr_value:.3f}", f"{p_chi_value:.3f}"])

# Print the table with centered text and bold headers
print(tabulate(table, headers="firstrow", tablefmt="grid", colalign=("center",)*5))

"""**AGE 18-24**"""

# Load data
data = pd.read_csv('Survey DATA Thesis - Clickbaits questions groups (8).csv')

filtered_data = data[data['v_1'] == 2]

print(filtered_data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column in the filtered data
num_columns = filtered_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Get descriptive statistics
filtered_data = data[data['v_1'] == 2]
desc_stats = filtered_data.describe()

# Determine the split point (halfway through the columns)
split_point = len(desc_stats.columns) // 2

# Split the descriptive statistics into two DataFrames
first_half_stats = desc_stats.iloc[:, :split_point]
second_half_stats = desc_stats.iloc[:, split_point:]

# Print the title for the table
print("Descriptive Statistics - Part 1")
# Print the first part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(first_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Add spacing between tables for readability
print("\n" + "-"*50 + "\n")

# Print the title for the second part of the table
print("Descriptive Statistics - Part 2")
# Print the second part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(second_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Assuming 'data' is your DataFrame and 'v_1' is the column of interest
value_counts = filtered_data['v_2'].value_counts()

# Filter to get only the counts of 1, 2, 3, 4, and 5
selected_counts = value_counts.loc[value_counts.index.isin([1, 2, 3])]

# Calculate percentages
total = filtered_data['v_2'].count()
percentages = (selected_counts / total) * 100

# Combine counts and percentages
combined = pd.DataFrame({
    'Counts': selected_counts,
    'Percentages': percentages
})

print(combined)

"""**AGE 25-34**"""

# Load data
data = pd.read_csv('Survey DATA Thesis - Clickbaits questions groups (8).csv')

filtered_data = data[data['v_1'] == 3]

print(filtered_data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column in the filtered data
num_columns = filtered_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Get descriptive statistics
filtered_data = data[data['v_1'] == 3]
desc_stats = filtered_data.describe()

# Determine the split point (halfway through the columns)
split_point = len(desc_stats.columns) // 2

# Split the descriptive statistics into two DataFrames
first_half_stats = desc_stats.iloc[:, :split_point]
second_half_stats = desc_stats.iloc[:, split_point:]

# Print the title for the table
print("Descriptive Statistics - Part 1")
# Print the first part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(first_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Add spacing between tables for readability
print("\n" + "-"*50 + "\n")

# Print the title for the second part of the table
print("Descriptive Statistics - Part 2")
# Print the second part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(second_half_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Assuming 'data' is your DataFrame and 'v_1' is the column of interest
value_counts = filtered_data['v_2'].value_counts()

# Filter to get only the counts of 1, 2, 3, 4, and 5
selected_counts = value_counts.loc[value_counts.index.isin([1, 2, 3])]

# Calculate percentages
total = filtered_data['v_2'].count()
percentages = (selected_counts / total) * 100

# Combine counts and percentages
combined = pd.DataFrame({
    'Counts': selected_counts,
    'Percentages': percentages
})

print(combined)

"""# **Chi-square statistic Testing between ages 18-24 and 25-34**

Age and Wie sehr lässt eine Werbeschlagzeile an der Qualität eines Spiels oder dem Vertrauen seines Entwicklers zweifeln?
"""

# Filter the DataFrame to include only rows where v_1 has values 2 or 3
df_filtered = data[data['v_1'].isin([2, 3])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_1'], df_filtered['v_18'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between ages (2 and 3) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Age and Wie gut können Sie Ihrer Meinung nach Clickbait-Schlagzeilen von allgemeinen Schlagzeilen in Anzeigen unterscheiden?"""

# Filter the DataFrame to include only rows where v_1 has values 2 or 3
df_filtered = data[data['v_1'].isin([2, 3])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_1'], df_filtered['v_16'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between ages (2 and 3) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Age and Wie oft spielen Sie bei Spielen um Geld oder schließen Wetten ab?"""

# Filter the DataFrame to include only rows where v_1 has values 2 or 3
df_filtered = data[data['v_1'].isin([2, 3])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_1'], df_filtered['v_26'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between ages (2 and 3) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Age and Wie risikofreudig würden Sie sich selbst einschätzen?"""

# Filter the DataFrame to include only rows where v_1 has values 2 or 3
df_filtered = data[data['v_1'].isin([2, 3])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_1'], df_filtered['v_23'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between ages (2 and 3) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""Age and Wie sehr würden Sie sich aufregen, wenn Sie einen Teil Ihres Geldes verlieren würden?"""

# Filter the DataFrame to include only rows where v_1 has values 2 or 3
df_filtered = data[data['v_1'].isin([2, 3])]

# Create a contingency table with the filtered data
contingency_table = pd.crosstab(df_filtered['v_1'], df_filtered['v_24'])

# Perform the Chi-square test of independence
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print(f"Chi-square statistic: {chi2_stat}, P-value: {p_value}")
print("Expected frequencies, if there is no relationship between ages (2 and 3) and response:\n", expected)

# Print the observed frequency table
print("\nObserved frequencies:")
print(contingency_table)

"""# Machine Learning"""

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Scaling the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Checking for missing values
imputer = SimpleImputer(strategy='mean')
imputed_data = imputer.fit_transform(scaled_data)

# Check the shape of the processed data
imputed_data.shape

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Determining the optimal number of clusters using the elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(imputed_data)
    wcss.append(kmeans.inertia_)

# Plotting the results
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') # Within cluster sum of squares
plt.grid(True)
plt.show()

from sklearn.decomposition import PCA


# Selecting the specific columns as mentioned
selected_columns = [
    "v_12",
    "v_13",
    "v_15",
    "v_17",
    "v_18",
    "v_16",
    "v_25",
    "v_26",
    "v_23",
    "v_24",
    "v_14",
]
selected_data = data[selected_columns]

# Scaling the selected data
scaler = StandardScaler()
scaled_selected_data = scaler.fit_transform(selected_data)

# Imputing any missing values in the selected columns
imputer = SimpleImputer(strategy='mean')
imputed_selected_data = imputer.fit_transform(scaled_selected_data)

# Performing k-means clustering on the selected data
kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)
cluster_labels_selected = kmeans.fit_predict(imputed_selected_data)

# Reducing the selected data to two dimensions using PCA for visualization
pca_selected = PCA(n_components=2)
reduced_selected_data = pca_selected.fit_transform(imputed_selected_data)

# Plotting the results
plt.figure(figsize=(10, 8))
plt.scatter(reduced_selected_data[:, 0], reduced_selected_data[:, 1], c=cluster_labels_selected, cmap='viridis', marker='o', edgecolor='k', s=70)
plt.title('Clusters of Survey Respondents (PCA-reduced Selected Data)')
plt.xlabel('PCA Feature 1: Game Engagement and Risk Tendency')
plt.ylabel('PCA Feature 2: Critical Perception of Game Advertising')
plt.grid(True)
plt.show()

# Selecting the specific columns as mentioned
selected_columns = [
    "v_12", "v_13", "v_15", "v_17", "v_18", "v_16", "v_25", "v_26", "v_23", "v_24", "v_14",
    # Add any other columns you need here
]
selected_data = data[selected_columns]

# Scaling the selected data
scaler = StandardScaler()
scaled_selected_data = scaler.fit_transform(selected_data)

# Imputing any missing values in the selected columns
imputer = SimpleImputer(strategy='mean')
imputed_selected_data = imputer.fit_transform(scaled_selected_data)

# Performing k-means clustering on the selected data
kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)
cluster_labels_selected = kmeans.fit_predict(imputed_selected_data)

# Cluster names based on their characteristics
cluster_names = {
    0: "Engaged and Uncritical Gamers",
    1: "Disengaged and Noncritical Gamers",
    2: "Engaged but Skeptical Gamers",
    3: "Disengaged but Critical Gamers"
}

# Mapping the cluster numbers to the names
cluster_labels_named = pd.Series(cluster_labels_selected).map(cluster_names)

# Calculating the counts and percentages for each named cluster
named_cluster_counts = cluster_labels_named.value_counts().sort_index()
named_cluster_percentages = named_cluster_counts / len(cluster_labels_named) * 100
print("Named cluster counts:\n", named_cluster_counts)
print("Named cluster percentages:\n", named_cluster_percentages)

# Reducing the selected data to two dimensions using PCA for visualization
pca_selected = PCA(n_components=2)
reduced_selected_data = pca_selected.fit_transform(imputed_selected_data)

# Plotting the results with named clusters
plt.figure(figsize=(10, 8))
scatter = plt.scatter(reduced_selected_data[:, 0], reduced_selected_data[:, 1], c=cluster_labels_selected, cmap='viridis', marker='o', edgecolor='k', s=70)

# Create a legend with the named clusters
plt.legend(handles=scatter.legend_elements()[0], labels=list(cluster_names.values()), title="Clusters")
plt.title('Clusters of Survey Respondents (PCA-reduced Selected Data)')
plt.xlabel('PCA Feature 1: Game Engagement and Risk Tendency')
plt.ylabel('PCA Feature 2: Critical Perception of Game Advertising')
plt.grid(True)
plt.show()

# Computing the PCA loadings (component weights) for the first two components
pca_loadings = pd.DataFrame(pca_selected.components_.T, index=selected_columns, columns=['PCA Feature 1', 'PCA Feature 2'])

# Display the loadings to understand the contribution of each original feature
pca_loadings

"""PCA Feature 1 Interpretation
High Loadings: The questions with the highest loadings on PCA Feature 1 are related to the respondents' engagement with games after clicking on advertisements, the fulfillment of game content as promised by headlines, beliefs about the reliability of game advertising with sensational headlines, frequency of gambling or betting in games, and overall risk-taking tendencies.
Low Loadings: Some questions, like those about gender or the reactions to financial loss, show lower or even negative loadings, suggesting less influence on this component.
Given these loadings, we can interpret PCA Feature 1 as representing a dimension related to the active engagement and interaction with games and game advertising. This component might capture aspects like:

High PCA Feature 1 Values: Indicating respondents who are more actively engaged with games, more likely to follow through on game ads, and potentially more open to or influenced by game advertising. This group might also exhibit higher tendencies towards risk-taking within gaming or betting contexts.
Low PCA Feature 1 Values: Suggesting respondents who are less engaged in these activities, perhaps more cautious or critical about following through on game advertisements, and possibly less inclined towards gambling or risk-taking in gaming.



PCA Feature 2
The highest loadings here are negative and come from questions about the importance of transparency and authenticity in game advertising, skepticism towards the quality of a game based on its headline, and the ability to differentiate clickbait headlines from general ones.
Thus, PCA Feature 2 appears to capture skepticism and critical attitudes towards game advertising and their headlines. High values in this component might represent respondents who place great importance on authenticity and are more discerning or skeptical about advertising claims.
Interpretation
High PCA Feature 2 Values: Likely represent respondents who are critical, skeptical, and value transparency in game advertising. They might be less susceptible to clickbait and more discerning about the quality and authenticity of what's advertised.
Low PCA Feature 2 Values: Might indicate respondents who are less concerned with these aspects or perhaps more accepting of or influenced by game advertising and its sensational headlines.

**CLUSTERS NAMES**

Cluster 1: "Engaged and Uncritical Gamers"

This cluster might consist of respondents who score high in "Game Engagement and Risk Tendency" but lower in "Critical Perception of Game Advertising". These are likely individuals who are actively engaged with gaming and susceptible to game advertising, without being overly critical or skeptical of the advertising content.
Cluster 2: "Disengaged and Noncritical Gamers"

Respondents in this cluster could be characterized by low scores in both "Game Engagement and Risk Tendency" and "Critical Perception of Game Advertising". These individuals might not be particularly engaged with gaming or influenced by game advertising, and they are also less critical or discerning about the authenticity and claims of such advertising.
Cluster 3: "Engaged but Skeptical Gamers"

These respondents could be high in both "Game Engagement and Risk Tendency" and "Critical Perception of Game Advertising". They are likely engaged and responsive to gaming and its advertising but maintain a critical, discerning view towards the advertising content, seeking authenticity and transparency.
Cluster 4: "Disengaged but Critical Gamers"

This group might show low engagement and risk-taking tendencies (low in "Game Engagement and Risk Tendency") but high critical and discerning attitudes towards game advertising (high in "Critical Perception of Game Advertising"). They are less likely to be influenced by game advertising due to their skepticism and demand for authenticity.

# **Kaggle Data Analysis**

All clikcbaits and not clikcbaits
"""

# Read the CSV file
data_kaggle = pd.read_csv('Clickbait Data Thesis - Cleaned Data.csv')
data_kaggle.head()

# Read the CSV file
data_kaggle = pd.read_csv('Clickbait Data Thesis - Cleaned Data.csv')

# Set display format to non-scientific for floats
pd.set_option('display.float_format', lambda x: '%.3f' % x)

# Print descriptive statistics
print(data_kaggle.describe())  # Get descriptive statistics

# Visualizing the distribution of each numerical column
num_columns = data_kaggle.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(data_kaggle[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Get descriptive statistics
desc_stats = data_kaggle.describe()


# Print the title for the table
print("Descriptive Statistics")
# Print the first part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(desc_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Set display format to non-scientific for floats
pd.set_option('display.float_format', lambda x: '%.2f' % x)

# Select numerical columns
num_columns = data_kaggle.select_dtypes(include=['int64', 'float64']).columns

print(data_kaggle.describe())

# Function to remove outliers based on IQR
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Filter out outliers from each numerical column
filtered_data = data_kaggle.copy()
for col in num_columns:
    filtered_data = remove_outliers(filtered_data, col)

# Visualizing the distribution of each numerical column after removing outliers
# Increase the figsize here
plt.figure(figsize=(20, 15))  # Adjusted size
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.gca().get_xaxis().set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))
    plt.gca().get_yaxis().set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))
    plt.tight_layout()

plt.show()

"""Just clickbaits"""

# Load data

filtered_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

print(filtered_data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column in the filtered data
num_columns = filtered_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Get descriptive statistics
filtered_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]
desc_stats = filtered_data.describe()

# Print the title for the table
print("Clickbait Descriptive Statistics")
# Print the first part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(desc_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Get descriptive statistics
filtered_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 0]
desc_stats = filtered_data.describe()

# Print the title for the table
print("Not Clickbait Descriptive Statistics")
# Print the first part of descriptive statistics with numbers formatted to three decimal places
print(tabulate(desc_stats, headers='keys', tablefmt='grid', floatfmt=".3f"))

# Filter to keep only rows where 'Clickbait 1/ Not-Clickbait 0' equals 1
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

# Set display format to non-scientific for floats
pd.set_option('display.float_format', lambda x: '%.2f' % x)

# Select numerical columns
num_columns = clickbait_data.select_dtypes(include=['int64', 'float64']).columns

# Function to remove outliers based on IQR
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Filter out outliers from each numerical column
filtered_data = clickbait_data.copy()
for col in num_columns:
    filtered_data = remove_outliers(filtered_data, col)

# Visualizing the distribution of each numerical column after removing outliers
plt.figure(figsize=(20, 15))  # Adjusted size
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.gca().get_xaxis().set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))
    plt.gca().get_yaxis().set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))
    plt.tight_layout()

plt.show()

"""Not Clickbaits"""

# Load data

filtered_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 0]

print(filtered_data.describe())  # get descriptive statistics

# Visualizing the distribution of each numerical column in the filtered data
num_columns = filtered_data.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.tight_layout()

plt.show()

# Filter to keep only rows where 'Clickbait 1/ Not-Clickbait 0' equals 0
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 0]

# Set display format to non-scientific for floats
pd.set_option('display.float_format', lambda x: '%.2f' % x)

# Select numerical columns
num_columns = clickbait_data.select_dtypes(include=['int64', 'float64']).columns

# Function to remove outliers based on IQR
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Filter out outliers from each numerical column
filtered_data = clickbait_data.copy()
for col in num_columns:
    filtered_data = remove_outliers(filtered_data, col)

# Visualizing the distribution of each numerical column after removing outliers
plt.figure(figsize=(20, 15))  # Adjusted size
for i, col in enumerate(num_columns, 1):
    plt.subplot(4, 5, i)
    sns.histplot(filtered_data[col], kde=True)
    plt.title(col)
    plt.gca().get_xaxis().set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))
    plt.gca().get_yaxis().set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))
    plt.tight_layout()

plt.show()

"""all data"""

correlation_matrix = data_kaggle.corr()

# Create a larger figure before generating the heatmap
plt.figure(figsize=(20, 18))

sns.heatmap(data = correlation_matrix, annot = True)
plt.show()

"""**DISLIKES COMPARED TO LIKES IN PERCENTAGE (Clickbaits)**"""

# Filter to keep only rows where 'Clickbait 1/ Not-Clickbait 0' equals 1
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

# Calculate total likes and dislikes
total_likes = clickbait_data['Likes'].sum()
total_dislikes = clickbait_data['Dislikes'].sum()

# Calculate proportions
total = total_likes + total_dislikes
proportion_likes = total_likes / total
proportion_dislikes = total_dislikes / total

# Plotting
labels = ['Likes', 'Dislikes']
sizes = [proportion_likes, proportion_dislikes]
colors = ['#ff9999','#66b3ff']

plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Proportion of Likes and Dislikes for Clickbait Content')
plt.show()

# Filter to keep only clickbait titles
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

# Calculate dislikes-to-likes ratio for each clickbait headline (in percentage)
clickbait_data['Dislikes_to_Likes_Percentage'] = (clickbait_data['Dislikes'] / clickbait_data['Likes']) * 100

# Calculate the average percentage
average_percentage = clickbait_data['Dislikes_to_Likes_Percentage'].mean()

# Display the average dislikes-to-likes percentage
print("Average Dislikes-to-Likes Percentage for Clickbait Headlines:", average_percentage)

"""**DISLIKES COMPARED TO LIKES IN PERCENTAGE (NOT Clickbaits)**

"""

# Filter to keep only rows where 'Clickbait 1/ Not-Clickbait 0' equals 0
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 0]

# Calculate total likes and dislikes
total_likes = clickbait_data['Likes'].sum()
total_dislikes = clickbait_data['Dislikes'].sum()

# Calculate proportions
total = total_likes + total_dislikes
proportion_likes = total_likes / total
proportion_dislikes = total_dislikes / total

# Plotting
labels = ['Likes', 'Dislikes']
sizes = [proportion_likes, proportion_dislikes]
colors = ['#ff9999','#66b3ff']

plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Proportion of Likes and Dislikes for NOT Clickbait Content')
plt.show()

# Filter to keep only clickbait titles
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 0]

# Calculate dislikes-to-likes ratio for each clickbait headline (in percentage)
clickbait_data['Dislikes_to_Likes_Percentage'] = (clickbait_data['Dislikes'] / clickbait_data['Likes']) * 100

# Calculate the average percentage
average_percentage = clickbait_data['Dislikes_to_Likes_Percentage'].mean()

# Display the average dislikes-to-likes percentage
print("Average Dislikes-to-Likes Percentage for NOT Clickbait Headlines:", average_percentage)

"""**LIKES COMPARED TO VIEWS IN PERCENTAGE (Clickbaits)**"""

# Filter to keep only rows where 'Clickbait 1/ Not-Clickbait 0' equals 1
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

# Calculate total likes and total views
total_likes = clickbait_data['Likes'].sum()
total_views = clickbait_data['Views'].sum()

# Calculate proportions
total = total_likes + total_views
proportion_likes = total_likes / total
proportion_views = total_views / total

# Plotting
labels = ['Likes', 'Views']
sizes = [proportion_likes, proportion_views]
colors = ['#ffcc99', '#99ff99']

plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Proportion of Likes to Views for Clickbait Content')
plt.show()

# Filter to keep only clickbait titles
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

# Calculate likes-to-views ratio for each clickbait headline
clickbait_data['Likes_to_Views_Ratio'] = clickbait_data['Likes'] / clickbait_data['Views']

# Calculate the average ratio
average_ratio = clickbait_data['Likes_to_Views_Ratio'].mean()

# Display the average likes-to-views ratio
print("Average Likes-to-Views Ratio for Clickbait Headlines:", average_ratio)

"""**LIKES COMPARED TO VIEWS IN PERCENTAGE (NOT Clickbaits)**"""

# Filter to keep only rows where 'Clickbait 1/ Not-Clickbait 0' equals 0
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 0]

# Calculate total likes and total views
total_likes = clickbait_data['Likes'].sum()
total_views = clickbait_data['Views'].sum()

# Calculate proportions
total = total_likes + total_views
proportion_likes = total_likes / total
proportion_views = total_views / total

# Plotting
labels = ['Likes', 'Views']
sizes = [proportion_likes, proportion_views]
colors = ['#ffcc99', '#99ff99']

plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Proportion of Likes to Views for NOT Clickbait Content')
plt.show()

# Filter to keep only clickbait titles
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 0]

# Calculate likes-to-views ratio for each clickbait headline
clickbait_data['Likes_to_Views_Ratio'] = clickbait_data['Likes'] / clickbait_data['Views']

# Calculate the average ratio
average_ratio = clickbait_data['Likes_to_Views_Ratio'].mean()

# Display the average likes-to-views ratio
print("Average Likes-to-Views Ratio for NOT Clickbait Headlines:", average_ratio)

"""**DISLIKES COMPARED TO VIEWS IN PERCENTAGE (Clickbaits)**"""

# Filter to keep only rows where 'Clickbait 1/ Not-Clickbait 0' equals 1
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

# Calculate total likes and total views
total_likes = clickbait_data['Dislikes'].sum()
total_views = clickbait_data['Views'].sum()

# Calculate proportions
total = total_likes + total_views
proportion_likes = total_likes / total
proportion_views = total_views / total

# Plotting
labels = ['Disikes', 'Views']
sizes = [proportion_likes, proportion_views]
colors = ['#ffcc99', '#99ff99']

plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Proportion of Likes to Views for Clickbait Content')
plt.show()

"""**DISLIKES COMPARED TO VIEWS IN PERCENTAGE (NOT Clickbaits)**"""

# Filter to keep only rows where 'Clickbait 1/ Not-Clickbait 0' equals 0
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 0]

# Calculate total likes and total views
total_likes = clickbait_data['Dislikes'].sum()
total_views = clickbait_data['Views'].sum()

# Calculate proportions
total = total_likes + total_views
proportion_likes = total_likes / total
proportion_views = total_views / total

# Plotting
labels = ['Disikes', 'Views']
sizes = [proportion_likes, proportion_views]
colors = ['#ffcc99', '#99ff99']

plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Proportion of Likes to Views for Clickbait Content')
plt.show()

"""**STATISTICAL TEST**"""

# Select the column for comparison, e.g., 'Clickbait 1/ Not-Clickbait 0'
target_column = 'Clickbait 1/ Not-Clickbait 0'

# Iterate through all other columns and test correlations
for col in data_kaggle.columns:
    if col != target_column and data_kaggle[col].dtype in ['float64', 'int64']:
        try:
            corr, p_value = pearsonr(data_kaggle[target_column], data_kaggle[col])

            # Check if absolute value of correlation is greater than 0.05
            if abs(corr) > 0.05:
                print(f"Correlation between {target_column} and {col}: Correlation coefficient = {corr}, P-value = {p_value}")
        except Exception as e:
            print(f"Could not calculate correlation for {col}: {e}")



"""**ML SENTIMENT ANALYSIS**

based on likes to views proportion from headliens that are clickbait
"""

# Filter to keep only clickbait titles
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

# Calculate likes-to-views ratio
clickbait_data['Likes_to_Views_Ratio'] = clickbait_data['Likes'] / clickbait_data['Views']

# Sort by likes-to-views ratio and select top 10
top_clickbaits = clickbait_data.sort_values(by='Likes_to_Views_Ratio', ascending=False).head(10)

# Set pandas display options to show full content
pd.set_option('display.max_colwidth', None)

# Display the top 10 clickbait titles
print("Top 10 Clickbait Titles Based on Likes-to-Views Ratio:")
print(top_clickbaits[['Video Title', 'Likes_to_Views_Ratio']])

# Filter to keep only clickbait titles
clickbait_data = data_kaggle[data_kaggle['Clickbait 1/ Not-Clickbait 0'] == 1]

# Calculate likes-to-views ratio for each clickbait headline
clickbait_data['Likes_to_Views_Ratio'] = clickbait_data['Likes'] / clickbait_data['Views']

# Calculate the average ratio
average_ratio = clickbait_data['Likes_to_Views_Ratio'].mean()

# Display the average likes-to-views ratio
print("Average Likes-to-Views Ratio for Clickbait Headlines:", average_ratio)

# Load the data from the file
clickbait_data = pd.read_csv('Clickbait Data Thesis - Cleaned Data.csv')

# Function to clean and split text into words
def clean_and_split(text):
    # Removing special characters and splitting into words
    words = re.findall(r'\w+', text.lower())
    return words

# Apply the function to each title and aggregate all words
all_words = sum(clickbait_data['Video Title'].apply(clean_and_split), [])

# Count the frequency of each word
word_counts = Counter(all_words)

# Display the most common words
print("Most Common Words:")
print(word_counts.most_common(20))

# Function to generate n-grams from a list of words
def generate_ngrams(words, n):
    return ngrams(words, n)

# Generate bi-grams and tri-grams
bigrams = generate_ngrams(all_words, 2)
trigrams = generate_ngrams(all_words, 3)

# Count the frequency of each bi-gram and tri-gram
bigram_counts = Counter(bigrams)
trigram_counts = Counter(trigrams)

# Display the most common bi-grams and tri-grams
most_common_bigrams = bigram_counts.most_common(10)
most_common_trigrams = trigram_counts.most_common(10)

print("\nMost Common Bi-grams:")
print(most_common_bigrams)

print("\nMost Common Tri-grams:")
print(most_common_trigrams)